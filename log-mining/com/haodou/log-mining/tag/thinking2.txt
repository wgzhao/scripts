1. 首要的要把现有的想法简化，想到能实现最关键想法的简单可行方案。
简单性，模块化很重要。也就是说，要能承受系统的架构的变化。
过去没写一点就写不下去，或者东西无法复用的情况要改变。

过去的方案每次浅尝辄止，问题不在于复杂性，而在于没有一个完整的实施方案，持续实施。


2.几个问题：
（1）一个灵活的存储系统，适应不同的知识来源，可以随时添加，方便检索。
存储系统要兼顾效率和灵活性，兼顾知识的深度。
兼顾实时运行和日志记录以及离线处理。
要考虑以后添加新的结构，考虑如何平滑过渡。

降低各数据块之间的依赖性。
私人定制数据生成模块，菜谱模型和用户模型依赖一个单独的词典。
如何降低这种依赖：[降低这种依赖的原因一是提高效率，其二是在发生词典丢失时也能很好运作。]
常用词的词典应该在各个模型内部自己拥有，可以自给自足。
非常用词的词典可以用外部词典，这时候依赖依然存在。但要有一套机制，在外部词典丢失时，有尽可能好的错误处理机制。

（2）各个环节的评估指标。
（3）
概念的表示。概念属性。实体概念在描述时的约束。
约束的表达和推理。
抽象概念的具体化，能够打通不同的抽象概念，融合到一个体系中。规则可以借助于这个体系加入，这个体系中的规则可以解析，可推导，可验证。
（4）人机交互的机制。可以很方便地管理和添加规则。方便地校验错误。

3. 有哪些要素：
（1）文件重复扫描的问题，首先由外部来控制。内部怎么知道是出现了重复文本，还是因为重复扫描。
重复模式的最低频次限制会首重复次数的影响。
在每次开启重复扫描前，要进行一次清理。
先手工控制这个问题。

当然，重复检测要同时开启。重复检测可以用来评估模式的上下文多样性。

（2）删除的条目要记录吗
可以作为日志记录起来。从内存中删除，但是在外存中记录。
一个模式现在出现次数不多，以后可能出现次数多吗？

现在的原则要尽量删减在内存中的条目。
写到日志中的记录也只做离线分析，不做在线查询。
在外存中记录可以便于人追查原因。

对于那些不佳的模式，及时丢弃。
何谓“不佳”，就是：
a. 上下文多样性不够（可以被长串包含）
b. 频次太低。
c. 本模式的频次与某子串的频次差异太大。
*d. 内部元素不在同一个层级，内部元素不完整。

(3)模板即概念，概念即模板。一切皆模板(tmplate).
a. 模板偏向于描述模板。
一个概念可以对应多个描述模板。
b. 模板到概念的映射可以看成是一个模板。
c. 都以模板的形式存在。不过，做一个汇总模板来表征一个概念。

不必维持一个完整的概念。修改起来会很麻烦。
而把概念的各个组成要素分拆。
这个做法类似谓词的二元化。
便于增删条目，修改。

不过，在后期，可以写程序，从这些分散的条目中恢复出完整的实体概念。

可以采取这样的方式：
不时暂停对文本的扫描，抽出一定的时间来做词典整理。在这个过程中，可以去掉一些不佳模式。
以后如果并行化了，则可以不用这样，可以用后台进程来做词典整理。
现在先做简单的方案。

为什么要抽出时间做整理。
这个是为了运用一些在外存上面的信息。
而外存上面的信息最好批量读取。
这时候，在线解析或者在处理一个词典（一个模块）时，都可以提问。当在处理别的词典（模块）时，这些问题的答案就找到了。这个答案记录到总线系统中，下次回头处理本模块时，填回答案。
这样，我就完全可以以一个单机系统实现深度智能处理。

采取这样一种模式：
为了下次扫描时方便，每个文段，要在后面附上已经查询到的词条统计项。这样，在下次扫描时，就可以直接使用已经查到的词条项，而不必发起新的查询请求。
而且，对于每次扫描所得的模板生成结果，也直接附在后面。
当词条的统计项发生较大变化时,则要发起广播。所有涉及该词条的文段后面所附的词条统计项都要相应修改。
为了便于并行，以上操作都在文件中进行。
另外，为了便于并行，文段和词条都视为同样的格式的东西，可以打散写入。


(4)规定一个通用的词典模式。
现在只有一种格式，后面再考虑多种格式。
约束也要化为模板，用同样的格式。

"白菜叶"->【食材】【食材部位】
已经上升为模板，那么该词语还需要在内存词典中吗？频次如果不够高，则可以不用了。
频次如果够高，考虑到路径合并，便于检索，可以在内存词典中。
【食材】【食材部位】->【食材】

为什么频次不够高则可以不用再存储。
因为在扫描时，将可以上升到模板。

如果频次足够高，则原字符串可以独立作为一个组成部分构成与其他成分构成新的模板。
频次不够高不等于低频。
频次不够高是指没有达到一个比同类字符串更高的抽象层级。同类字符串，是指同一个模板下的其他字符串。

简单地说，如果“白菜叶”在“[食材][食材部位]”中的集合中没有排到前列，那么就无需单独列在词典中了。
另外，还有一个前提，因为这里“白菜叶”中的要素被分解了，也就是说“白菜叶”可以通过“[食材]【食材部位】”中的槽位填充来恢复。如果无法通过槽位填充来恢复，则“白菜叶”不可以省略。

AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAAAAAAAAAA
这里会导致“AAA”的数目有几十个。
这个现在要特殊处理么。最好能够马上解析模板。那样，就避免统计很多无效模式了。

一个模板如果包括一个元素(设为A)的n份copy，则不统计该n份copy的子串。
但是统计A+,A*。可以划分一些阶段。A*2,A*4,A*8,A*16这样的，表明重复2、4、8、16次以上。


模板的内容：召回规则(基于上下文的召回规则，基于内部元素的召回规则)，判定规则，判定例外规则，约束条件，频次，必须元素，必须元素下模板召回率和准确率。
所属类别及类名。元素列表。元素重复模式。近似模板编号。典型例句。文章背景。
模板如果是描述模板，则对归类后的实体概念的属性会进行填充。
那么，对于实体概念的属性填充的取值要在模板里面标识么。

如果将所属类别看成是一个属性而已，则不存在那个问题了。
问题：要区分属性和元素么。现在最好区分。而且，在人的心里，的确有一个标杆。
元素是组成部分，可以被分割，被部分匹配。
属性则是整体属性。
不可以部分匹配，“通常”只可以整体匹配。

属性候选生成，重要性判定，属性生成。
模板候选生成，重要性判定，模板生成。
属性本身可以看成是一种模板。
一个模板可以作为另外一个模板的属性。

当一个模板有多个属性时，属性之间的关系要怎么清理。这个问题暂时不好解答。我们不知道机器会得出一些什么样的属性出来。
属性模板与一般的模板（描述模板）有很大差别。描述模板中的元素都是文章中可见的，属性模板中的元素则通常是隐含的。
属性模板本身更像一个概念。

在将一个模板嵌入到一个更大的模板中时，该选取该模板的哪些属性。这个可以作为一个分类问题。

模板的属性应该可以随意加入。【这个就像谓词二元化所带来的好处一样。这样也便于我们加入新的数据。便于数据的管理。我们不需要考虑现有的知识，可以随意加入知识。
加入的知识之间关系的处理由程序来把握。难点在这里。虽然用户添加知识时可以不用管过去的知识，电脑还是要把新知识和旧知识之间的关系告诉给用户。】


4. 抽象概念与推理系统的无缝接合。
（1）因果关系的阐释：一个概念的两个要素之间存在相关关系。比如，木耳是干的，其卟啉含量就少。木耳是鲜的，则含量就高。
这种约束关系在做自然语言处理时，不需要推导，只要解析就可以。
那么，在描述时，两个要素所共处的概念应该是被讨论的对象。
(2) 不过，很多时候，因果关系中要素所处的概念并不清楚。
“因为生病了，所以没有来上学。”--------“生病”和“上学”是两个概念。另要把两个因素放到一起形成一个新的概念模板，则会使得概念空间复杂化，反而不利于高效地推理。
不过，找两个概念共同作用的对象是有必要的，“生病”和“上学”的共同对象都是当事人。
(3)也就是说，因果关系直接与概念之间的约束关系是对应的。
因果关系是一种知识库层面的概念，不同于一般的概念。
自然语言处理中的这种多层级性。

5. 关于异常的解释问题：
如果99.9%的A出现在B的上下文中，但是有一个A出现在没有B的上下文中。那么，这个没有B的上下文，哪个因素才是关键。
需要把这个上下文详细记录下来，供人类专家来提取有效因素。因为都有B这种很好提取。但是，没有B的情况属于个案，因此，其因素的自动提取就很困难。
首选，要提取与B概念相近的因子。也就是B的变体，首先要检索B有哪些可能的变体。如果在B上没有记录，那么检索与B类似的概念又有哪些变体。

6. 关于“词语”的定义：
长度的波动范围。
已知常见词语。
分词器。
是一个描述层面的概念。“描述层面的概念”又是一个什么概念。在系统里面该怎么表示。
子类包括动词，名词，形容词，副词等。
从与上下文的关系来看：词语内部结合紧密（通常mi值高），词语外部上下文有一定的多样性。如果外部没有多样性，那么那个上下文本身是一个抽象层级更高的词语。
上下文有多样性，意味着上下文对应槽位上的字符串可以用同类的词语来替换。
如果字符串用于作为一个实体概念的名称，则通常该字符串可以看成是一个词语。
从我们的系统来看，不能泛化为模板的字符串都可以称为词语。都当做词语来看。
但是，对于“英国”这样的词语，内部还是可以解析模板的。
内部不能泛化为模板的，且与上下文结合后可以泛化成模板的，可以称为词语。

另外，这个概念应该要与人类专家进行交流的。
要确定一个上界和下界，什么样的情形大家都认为不是词语，什么情形都认为一定是词语，什么情形是两可的。
在某些情况下，我们只需要一个广义的概念，某些情况下，却需要一个狭义的概念。这个可以让机器自己去学习。
也就是说，对词语的解析分若干种模板，有些定义可以覆盖所有模板，有些定义只覆盖部分模板。

为什么一定要花时间讨论一下“词语”这个概念，因为我们很多的规则都要基于词语来表达。
比如词性，而词性对于模板生成以及概念槽位的推理都很重要吧。


词语内部的字符之间紧密度要高于与外部字符之间的紧密度。

7. 属性的向量表示。
如果一个模板有很多属性，如果所有属性都用原型表示的话，则会占用很多空间。
可以将多个属性压缩到一个向量中。
那么，为了提高解析效率，对向量含义进行定义的内容就要常驻内存，而且要以极高的置信度来存储，确保其准确性和完整性。

也可以说，多个属性合并后，形成了一个新的复合属性。
这个新的复合属性本身可以看成一个概念或者模板。


8. 怎么避免“牛肉”和“，”这种共现被当做重要模式。
因为语料太单一，无法分离“牛肉”和“，”的抽象层级。
既然我们是特定语料，那么无法分离也不要紧。
只是这样的模式在向上归纳时，或者放到更大的语料范围中，可以被回退。

不过，如果向上归纳层级很多时，这种回退也变得很复杂。

9. "肥肉部分切丝后改刀成石榴粒"中的“改刀”是什么含义。
第一层理解：是理解为一种加工手段就好。
第二层理解：“切”和“改刀”是连续动作。
第三层理解：“改刀”是“切”的结果上进行加工，那么，“石榴粒”也是从”丝“变过来。
第四层理解：“丝”为什么可以变成“石榴粒”形状。
第五层理解：为何要切”石榴粒“，这个背后的理由是什么。
为了加深理解层次，就要多问几个为什么。可以定义哪些问为什么的规则。

10. 关于扩展法：
有人想用扩展法来确定汉语的词。一个语言格式AB，如果能插进入一个C去，扩展成ACB，那就证明A和B是彼此独立的。例如“买书”可以插入“一本”，扩展成“买一本书”，“白纸”可以插入“的”。
也就是，对于语法的各种变体，本来不会导致语义变化的，但是，如果对于某些词语做这个操作，则会引起语义上的变化。
当然，并不是所有的词语都满足这个约束，比如“理发”就不满足这个要求。

对于人类专家来说，他们很容易判定扩展之后的语义是否一致，但是对于电脑却比较困难。
不过，我们可以通过对比不同变体的分布就可以。
像“白菜”这样的，“白的菜”基本上不会出现在文章语料中。

11. 
发现模式，这个很重要。
如“糊里糊涂”，“古里古怪”，怎么发现他们与“糊涂”，“古怪”的关系。
这个很不容易。

又怎么总结“AABB”和“ABAB”这样的重叠式与词性的关系。

12. 
如果要引入一些抽象模式来解释语言现象，则会有很多种可能的解释。
在缺乏外部指导的情况下，当然要用好剃刀原理。

不过，在特定领域中符合剃刀原理的东西，放眼到整个语言中则不一定符合。
这就是在语料上必须要做的文章。近期要收集足够的语料。
其实，首要是收集适合人类基础知识的语料。

13.补充元素在构成元素之间的位置发生变化后，整个句子的意思不发生变化，表明补充元素的抽象层次至少与构成元素相等，甚至可以更高。
N0 Aux V N1 Prep N2
a typical sentence form with an auxiliary and two complements, most
inserts may occur either at the beginning or at the end of the sentence, or at
any
of the four spaces separating the constituents:
Without any reason, the Dow Jones has lost 100 points at 3.000
The Dow Jones, without any reason, has lost 100 points at 3.000
The Dow Jones has, without any reason, lost 100 points at 3.000
etc.


14. 分批切换：
可以区分训练集和测试集。
也便于程序模块化。在进行批量切换时，可以从一个模块切换到另一个模块。
批量处理比逐条处理有更大的世界，比如在确定某个词语是否是新词的时候。必须累积一定的量。

15. 将语法框架跟概念框架对应起来：
名词，主语，宾语通常是概念。
而谓语通常是对概念的属性进行描述。
名词也可以作为概念的属性名，如“雪的颜色是白色的”。这里的“颜色”是属性名。
整个“雪的颜色”又可以看成是一个概念。
合并模板，表示模板表示同一件事情。


16.
几个独立操作，对现有模式在上下文中进行扩展，形成新的模式候选。
对模式进行筛选。

几种基本关系的定义及其判定：
相似关系，含相同关系。
集合关系。包含关系。任一。某些。补集。空集。
有序关系。数量关系是一种有序关系。----当然，数量关系也是要特别定义的。
概念-属性名值-实体关系：概念是概念，而实体则是外延。
整体部分关系：整体只能是实体，而不能是概念。


整体和部分的关系：是一种集合关系。是一种包含关系。多个元素构成一个整体（实体）。
ISA关系：也是一种集合关系。把整个类的实体看成集合，单个实体就被包含于这个集合中。


基于基本关系定义更多的关系：
代词和原词之间的关系是代表模板内部元素之间的相同关系。
要定义一个相同关系，就是指向的对象相同。
相同关系是相似关系的一种。



17.
它，他，她，分人和事物。
“这”，“那”单独做主语时，可指人，也可指事物。但是，做宾语时，只可以指人。

可见“人”和“事物”的区分在代词中很重要。


18.
“牛肉、羊肉都是畜肉。”
这样的句子是提出召回假设的地方。“牛肉”和“羊肉”是同一个类的实例。
不过，类别有大有小，并列的项目虽然通常属于较小的类，但也不乏较泛的类。

怎么验证牛肉和羊肉的确是很类似的项目呢。
可以比较两者的内部结构。

召回特征和准确特征最大的区别不在于谁的召回率高，谁的准确率高。
而在于召回特征更容易被识别。
召回特征可以遵循“不含目标概念的特征-->目标概念”。

也就是说，召回特征是用于提出问题的。
而准确特征是在给定问题的情况下，用于回答问题的。
给定目标概念，判定是不是存在目标概念。
默认假设，目标概念是特殊的，默认是不存在的。

召回特征在打破默认假设，从而提出问题。
准确特征要做一个统计判定，确认是拒绝假设，还是接受假设。

应该说，召回特征是强特征。
就像一个证据一样，证明你有罪了，要么，你需要否定这个证据，要么你就排除这个证据。你不可以无视这个证据。
召回特征就是这样的证据。

20. 从系统简化的角度而言，目标概念的定义都不必去死抠其真正的含义。
比如，IS-A关系和HAS-A关系的真正区别是什么。
要定义区别，也要从语言推导的角度来定义。

语言推导是可以从字符串层面定义的，且从字符串层面可以验证的推导关系。
比如“切”后面往往跟“【食材】”这个是可以推导，可以验证的。

21. 在训练过程中，要随时注意评估哪些元素已经被充分训练了，值得信任。
如果要在权值上做调整，必须是别的权值，而不是这些元素上的权值。

Word2vec中对高频词语的处理，就是符合这个原则的：
Subsampling of FrequentWords
In very large corpora, the most frequent words can easily occur hundreds of
millions of times (e.g.,“in”, “the”, and “a”). Such words usually provide less information value than
the rare words. For example, while the Skip-gram model beneﬁts from observing the co-occurrences
of “France” and “Paris”, it beneﬁts much less from observing the frequent co-occurrences of
“France” and “the”, as nearly every word co-occurs frequently within a sentence with “the”. This idea
can also be applied in the opposite direction; the vector representations of frequent words do not
change signiﬁcantly after training on several million examples.


22. 每个概念有取值范围，变动范围。
每个概念在表达上也有字符串层面的取值范围。
我们要建立概念，使得我们能够精确对应槽位上的取值范围。取值范围越精确，效果越好。取值范围越精确，则每个预测准确的项目的概念越大，整个文本的概率也就越大。
这个跟最大化语言模型的目标是一致。

不过传统的语言模型有一个简洁的概率计算公式。
如果我们要引入一个层级的框架，则这个概率很难计算。
不同层次的东西的概率怎么归一到一起。

一个简单的办法是挖一法，就是把每个位置挖空，看如果根据所有其他部分来预测这个位置，看概率如何。
在挖空后，只需要列出所有可能的候选及其概率。

不过有些取值范围是连续值，不可穷举，这个位置的预测概率与其他位置的预测概率无法连乘。

23. 对齐模板的解码剪枝：【没看懂】
The parser only operates on the French-side grammar;
the English-side grammar affects parsing only
by increasing the effective grammar size, because
there may be multiple rules with the same French
side but different English sides, and also because intersecting
the language model with the English-side
grammar introduces many states into the nonterminal
alphabet, which are projected over to the French
side. Thus, our decoder’s search space is many times
larger than a monolingual parser’s would be. To reduce
this effect, we apply the following heuristic
when filling a cell: if an item falls outside the beam,
then any item that would be generated using a lowerscoring
rule or a lower-scoring antecedent item is
also assumed to fall outside the beam. This heuristic
greatly increases decoding speed, at the cost of some
search errors.


24. IBM模型中，为什么将词对齐作为模型1，然后将位置对齐作为模型2。
因为我们人知道，词对齐是因果关系，而位置对齐是相关关系。
这种对于元素关系的先验知识，很有用。

25. 质变到量变。
(1)如果把连续变量离散化，则往往意味着划分质变和量变的界限。
跟模型的训练对应起来，质变意味着模型的变迁，或者hyperparameter的变化。
而量变则是模型内部的参数的值的调整。
可以将样本内部的量变与模型的参数的值调整对应起来。
而目前多数的算法中，则通常只有样本的质变才能与模型的参数的值的调整对应起来。

通常，特征是连接样本和目标概念的，特征的取值是布尔型。但是，特征对应的参数却是连续性，这使得特征本身有量变的特性。
如果特征不描述关联，而只描述样本本身的属性，则可以直接用连续值？这时候是否应该将样本本身视为目标概念？
如果特征要与目标概念关联，但是这个连续值怎么与参数的连续性整合。

(2)质变与量变更重要的应用场景不是特征和分类器的关系。而是分类器的应用场景。
分类器应该判断当前的场景，是否发生了质变，因而在模型上进行切换。
目前的分类器并没有内置这样的hyper
parameter，而所有的特征参数都是对等的，是局部的，都是量变层面的，不能促成质变。



